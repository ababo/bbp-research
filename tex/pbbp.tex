\documentclass{article}

\usepackage{amsmath, amsthm, hyperref}

\title{Practical Boolean Backpropagation}
\author{Simon Golbert}
\date{\today}

\setlength{\parskip}{5pt}
\linespread{1.1}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\begin{document}

\maketitle

\begin{abstract}
    Boolean neural networks offer hardware-efficient alternatives to real-valued models. While quantization is common, purely Boolean training remains underexplored. We present a practical method for purely Boolean backpropagation for networks based on a single specific gate we chose, operating directly in Boolean algebra involving no numerics. Initial experiments confirm its feasibility.
\end{abstract}

\section{Introduction}
To reduce the computational complexity and memory requirements of models, various quantization techniques are widely used today. Floating-point numbers are typically reduced to 16-, 8-, or 4-bit representations. This technique allows preserving the traditional gradient-based optimization of a differentiable loss function through backpropagation.

More recent research is moving toward more extreme forms of quantization. For instance, \cite{ma2024} introduces BitNet b1.58, a novel 1.58-bit Large Language Model (LLM) where each parameter is represented using ternary values {-1, 0, 1}. Another prominent study, \cite{rastegari2016}, presents XNOR-Net, a method for efficient image classification using purely Boolean convolutional neural networks. Despite achieving very descent inference efficiency, the latter project (like the former) relies on training driven by traditional backpropagation (albeit with heavily quantized parameter values), thus still requiring significant computational resources to run.

To maximize efficiency in both model training and inference, we should consider a methodology based on purely Boolean computations. This approach has the potential to leverage existing hardware for massive parallelism while also opening avenues for developing specialized, cost-effective, and energy-efficient hardware optimized for bitwise operations. While not yet a definitive solution, exploring this direction could lead to significant advancements in high-performance computing.

In this article, we will define a composite Boolean gate to serve as a neuron in Boolean artificial neural networks (ANNs). Additionally, we will derive an error backpropagation routine to enable training. Finally, we will discuss practical aspects of the proof-of-concept (PoC) implementation and share key observations from our experiments.

\section{Model Structure}
We define the gate function, the related \textbf{Row Activation} operation, and the inference process for a simple model composed of fully connected layers.

\subsection{Gate Function}
This research is focused on utilizing the following composite gate as a neuron function:
\[ y = \bigvee_{i} \left( x_{i} \wedge w_{i} \right) \oplus b \]

Here \(x_{i}\) are the argument values, \(w_{i}\) and \(b\) are parameter values that are learned during training.

\textit{Note: To save space, from now on, we will use 1 instead of True and 0 instead of False to represent the Boolean values throughout the document.}

It's easy to see that the gate above is complete. Indeed we can  express the OR, AND, and NOT gates using this gate:
\[ NOT(x) = \left( x \wedge 1 \right) \oplus 1 \]
\[ OR(x_1, x_2) = \left(\left( x_1 \wedge 1 \right) \vee \left( x_2 \wedge 1 \right) \right) \oplus 0 \]
\[ AND(x_1, x_2) = \left(\left( NOT(x_1) \wedge 1 \right) \vee \left( NOT(x_2) \wedge 1 \right) \right) \oplus 1 \]

\subsection{Row Activation}
In order to express model inference in matrix form using the gate function defined above, we introduce the following operation.

\begin{definition}
    \textbf{Row Activation} is an operation that is defined for two Boolean matrices X and W of size \( m \times n \), denoted as:
    \[ Z = \mathcal{A}(X, W) \]
    and produces a \( 1 \times m \) Boolean row vector \( Z \). Each element \( Z_{1i} \) (for \( i = 1, 2, \dots, m \)) is computed as:
    \[ Z_{1i} = \bigvee_{j=1}^{n} \left( X_{ij} \wedge W_{ij} \right). \]

    In other words, \( Z_{1i} \) is true if there exists at least one column \( j \) such that both \( X_{ij} \) and \( W_{ij} \) are true.

    The \textbf{Row Activation} operation can also be applied when one of the arguments (either \( X \) or \( W \)) is a \( 1 \times n \) Boolean row vector, while the other remains an \( m \times n \) matrix. In this case, the vector is broadcast to an \( m \times n \) matrix by repeating it across all \( m \) rows before applying the standard row activation operation.
\end{definition}

\subsection{Fully Connected Layer}
Now we can define a fully connected layer as \( (W, B) \), where \( W \) is a matrix of "weights" and \( B \) is a vector of "biases". For the input vector X, the output of the layer is computed as:
\[ Y = \mathcal{A}(X, W) \oplus B \]

Here, \( B \) is a \( 1 \times m \) Boolean row vector that selectively inverts elements of the \textbf{Row Activation} output via elementwise XOR.

A simple model can be represented as a series of such layers, where during inference, the output of the \( i \)-th layer serves as the input to the \( (i+1) \)-th layer.

\section{Model Training}
In this section, we introduce key definitions that will aid in outlining a training process, which conceptually resembles traditional error backpropagation.

\subsection{Activation Sensitivity}
To direct the training process, we need to determine which elements contribute to the final result and which do not. This concept is similar to the gradient in traditional training, but instead of quantifying influence, Boolean values indicate whether a change in an element flips the final output.

For instance, consider the following \textbf{Row Activation} application:

\[
    X = \begin{bmatrix} 1 & 0 & 1 & 0 \end{bmatrix},\quad
    W = \begin{bmatrix} 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \end{bmatrix},\quad
    Z = \mathcal{A}\left( X, W \right) = \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}
\]

Now, examine which elements of \( X \) need to be flipped to change each element of the result. It is evident that flipping any element of \( X \) cannot affect \( Z_{1,1} \), since \( W_1 \) is zeroed. Flipping \( X_2 \) or \( X_4 \) can change \( Z_2 \) from 0 to 1. To change \( Z_3 \) from 1 to 0, both \( X_1 \) and \( X_3 \) must be flipped.

An important observation that can be distilled from the example above is that flipping a resulting element from 0 to 1 requires flipping \textbf{any} of the relevant argument elements, while flipping a resulting element from 1 to 0 requires flipping \textbf{all} of the relevant arguments.

Now we can define operations that determine such "sensitive" elements.

\begin{definition}
    For given Boolean matrices \( A \) and \( B \) of size \( m \times n \), and \( Z = \mathcal{A}(A, B) \) the \textbf{Positive Activation Sensitivity} operation, denoted as
    \[
        S = \mathcal{S}^+(A, B),
    \]
    produces a matrix \( S \) of the same size, where each element \( S_{i,j} \) is defined as:

    \[
        S_{i,j} =
        \begin{cases}
            1, & \text{if } Z_{1,i} = 0 \text{ and setting } A_{i,j} = 1 \text{ causes } Z_{1,i} \text{ to flip to } 1, \\
            0, & \text{otherwise}.
        \end{cases}
    \]
    The \textbf{Negative Activation Sensitivity} operation, denoted as
    \[
        S = \mathcal{S}^-(A, B),
    \]
    produces a matrix \( S \) of the same size, where each element \( S_{i,j} \) is defined as:

    \[
        S_{i,j} =
        \begin{cases}
            1, & \text{if } Z_{1,i} = 1 \text{ and setting } A_{i,j} = 0 \text{ is necessary for } Z_{1,i} \text{ to flip to }  0, \\
            0, & \text{otherwise}.
        \end{cases}
    \]
    The \textbf{Activation Sensitivity} operation is defined as
    \[
        \mathcal{S}(A, B) = \mathcal{S}^+(A, B) \vee \mathcal{S}^-(A, B)
    \]
    The operations above can also be applied when one of the arguments (either \( A \) or \( B \)) is a \( 1 \times n \) Boolean row vector, while the other remains an \( m \times n \) matrix. In this case, the vector is broadcast to an \( m \times n \) matrix by repeating it across all \( m \) rows before applying the standard row activation operation.
\end{definition}

In the example above:
\[
    \mathcal{S}(X, W) = \begin{bmatrix} 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \end{bmatrix},\quad
    \mathcal{S}(W, X) = \begin{bmatrix} 1 & 0 & 1 & 0 \\ 1 & 0 & 1 & 0 \\ 1 & 0 & 1 & 0 \end{bmatrix},
\]

\section{Discussion}
Interpret results, compare with previous work, and explain implications.

\section{Conclusion}
Summarize the key points and suggest future directions.

\begin{thebibliography}{9}

    \bibitem{ma2024}
    Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei, \textit{The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits}, arXiv, 2024.

    \bibitem{rastegari2016}
    Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi, \textit{XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks}, arXiv, 2016.

\end{thebibliography}

\end{document}
