\documentclass{article}

\usepackage{amsmath, amsthm, hyperref}

\title{Practical Boolean Backpropagation}
\author{Simon Golbert}
\date{\today}

\setlength{\parskip}{5pt}
\linespread{1.1}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\theoremstyle{remark}
\newtheorem*{note}{Note}

\begin{document}

\maketitle

\begin{abstract}
    Boolean neural networks offer hardware-efficient alternatives to real-valued models. While quantization is common, purely Boolean training remains underexplored. We present a practical method for purely Boolean backpropagation for networks based on a single specific gate we chose, operating directly in Boolean algebra involving no numerics. Initial experiments confirm its feasibility.
\end{abstract}

\section{Introduction}
To reduce the computational complexity and memory requirements of models, various quantization techniques are widely used today. Floating-point numbers are typically reduced to 16-, 8-, or 4-bit representations. This technique allows preserving the traditional gradient-based optimization of a differentiable loss function through backpropagation.

More recent research is moving toward more extreme forms of quantization. For instance, \cite{ma2024} introduces BitNet b1.58, a novel 1.58-bit Large Language Model (LLM) where each parameter is represented using ternary values {-1, 0, 1}. Another prominent study, \cite{rastegari2016}, presents XNOR-Net, a method for efficient image classification using purely Boolean convolutional neural networks. Despite achieving very descent inference efficiency, the latter project (like the former) relies on training driven by traditional backpropagation (albeit with heavily quantized parameter values), thus still requiring significant computational resources to run.

To maximize efficiency in both model training and inference, we should consider a methodology based on purely Boolean computations. This approach has the potential to leverage existing hardware for massive parallelism while also opening avenues for developing specialized, cost-effective, and energy-efficient hardware optimized for bitwise operations. While not yet a definitive solution, exploring this direction could lead to significant advancements in high-performance computing.

In this article, we will define a composite Boolean gate to serve as a neuron in Boolean artificial neural networks (ANNs). Additionally, we will derive an error backpropagation routine to enable training. Finally, we will discuss practical aspects of the proof-of-concept (PoC) implementation and share key observations from our experiments.

\section{Model Structure}
We define the gate function, the related \textbf{Row Activation} operation, and the inference process for a simple model composed of fully connected layers.

\subsection{Gate Function}
This research is focused on utilizing the following composite gate as a neuron function:
\[ y = \bigvee_{i} \left( x_{i} \wedge w_{i} \right) \oplus b \]

Here \(x_{i}\) are the argument values, \(w_{i}\) and \(b\) are parameter values that are learned during training.

\begin{note}
    To save space, from now on, we will use 1 instead of True and 0 instead of False to represent the Boolean values throughout the document.
\end{note}

It's easy to see that the gate above is complete. Indeed we can  express the OR, AND, and NOT gates using this gate:
\[ NOT(x) = \left( x \wedge 1 \right) \oplus 1 \]
\[ OR(x_1, x_2) = \left(\left( x_1 \wedge 1 \right) \vee \left( x_2 \wedge 1 \right) \right) \oplus 0 \]
\[ AND(x_1, x_2) = \left(\left( NOT(x_1) \wedge 1 \right) \vee \left( NOT(x_2) \wedge 1 \right) \right) \oplus 1 \]

\subsection{Row Activation}
In order to express model inference in matrix form using the gate function defined above, we introduce the following operation.

\begin{definition}
    \textbf{Row Activation} is an operation that is defined for two Boolean matrices X and W of size \( m \times n \), denoted as:
    \[ Z = \mathcal{A}(X, W) \]
    and produces a \( 1 \times m \) Boolean row vector \( Z \). Each element \( Z_{1i} \) (for \( i = 1, 2, \dots, m \)) is computed as:
    \[ Z_{1i} = \bigvee_{j=1}^{n} \left( X_{ij} \wedge W_{ij} \right). \]

    In other words, \( Z_{1i} \) is true if there exists at least one column \( j \) such that both \( X_{ij} \) and \( W_{ij} \) are true.

    The \textbf{Row Activation} operation can also be applied when one of the arguments (either \( X \) or \( W \)) is a \( 1 \times n \) Boolean row vector, while the other remains an \( m \times n \) matrix. In this case, the vector is broadcast to an \( m \times n \) matrix by repeating it across all \( m \) rows before applying the standard row activation operation.
\end{definition}

\subsection{Fully Connected Layer}
Now we can define a fully connected layer as \( (W, B) \), where \( W \) is a matrix of "weights" and \( B \) is a vector of "biases". For the input vector X, the output of the layer is computed as:
\[ Y = \mathcal{A}(X, W) \oplus B \]

Here, \( B \) is a \( 1 \times m \) Boolean row vector that selectively inverts elements of the \textbf{Row Activation} output via elementwise XOR.

A simple model can be represented as a series of such layers, where during inference, the output of the \( i \)-th layer serves as the input to the \( (i+1) \)-th layer.

\section{Model Training}
In this section, we introduce key definitions that will aid in outlining a training process, which conceptually resembles traditional error backpropagation.

\subsection{Activation Sensitivity}
To direct the training process, we need to determine which elements contribute to the final result and which do not. This concept is similar to the gradient in traditional training, but instead of quantifying influence, Boolean values indicate whether a change in an element flips the final output.

For instance, consider the following \textbf{Row Activation} application:

\[
    X = \begin{bmatrix} 1 & 0 & 1 & 0 \end{bmatrix},\quad
    W = \begin{bmatrix} 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \end{bmatrix},\quad
    Z = \mathcal{A}\left( X, W \right) = \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}
\]

Now, examine which elements of \( X \) need to be flipped to change each element of the result. It is evident that flipping any element of \( X \) cannot affect \( Z_{1,1} \), since \( W_1 \) is zeroed. Flipping \( X_2 \) or \( X_4 \) can change \( Z_2 \) from 0 to 1. To change \( Z_3 \) from 1 to 0, both \( X_1 \) and \( X_3 \) must be flipped.

An important observation that can be distilled from the example above is that flipping a resulting element from 0 to 1 requires flipping \textbf{any} of the relevant argument elements, while flipping a resulting element from 1 to 0 requires flipping \textbf{all} of the relevant arguments.

Now we can define operations that determine such "sensitive" elements.

\begin{definition}
    For given Boolean matrices \( A \) and \( B \) of size \( m \times n \), and \( Z = \mathcal{A}(A, B) \) the \textbf{Positive Activation Sensitivity} operation, denoted as
    \[
        S = \mathcal{S}^+(A, B),
    \]
    produces a matrix \( S \) of the same size, where each element \( S_{i,j} \) is defined as:

    \[
        S_{i,j} =
        \begin{cases}
            1, & \text{if } Z_{1,i} = 0 \text{ and setting } A_{i,j} = 1 \text{ causes } Z_{1,i} \text{ to flip to } 1, \\
            0, & \text{otherwise}.
        \end{cases}
    \]
    The \textbf{Negative Activation Sensitivity} operation, denoted as
    \[
        S = \mathcal{S}^-(A, B),
    \]
    produces a matrix \( S \) of the same size, where each element \( S_{i,j} \) is defined as:

    \[
        S_{i,j} =
        \begin{cases}
            1, & \text{if } Z_{1,i} = 1 \text{ and setting } A_{i,j} = 0 \text{ is necessary for } Z_{1,i} \text{ to flip to }  0, \\
            0, & \text{otherwise}.
        \end{cases}
    \]
    The \textbf{Activation Sensitivity} operation is defined as
    \[
        \mathcal{S}(A, B) = \mathcal{S}^+(A, B) \vee \mathcal{S}^-(A, B)
    \]
    The operations above can also be applied when one of the arguments (either \( A \) or \( B \)) is a \( 1 \times n \) Boolean row vector, while the other remains an \( m \times n \) matrix. In this case, the vector is broadcast to an \( m \times n \) matrix by repeating it across all \( m \) rows before applying the standard row activation operation.
\end{definition}

In the example above:
\[
    \mathcal{S}(X, W) = \begin{bmatrix} 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \end{bmatrix},\quad
    \mathcal{S}(W, X) = \begin{bmatrix} 1 & 0 & 1 & 0 \\ 1 & 0 & 1 & 0 \\ 1 & 0 & 1 & 0 \end{bmatrix},
\]

It's easy to see that for a fully connected layer \( \mathcal{S}(X, W) \) and \( \mathcal{S}(W, X) \) define the sensitivity of Y to changes in X or W, respectively, regardless of B. Indeed flipping z always flips \( z \oplus b \) regardless of the value of b.

\subsection{Error Projection}
Before formally defining the \textbf{Error Projection} operation, we first develop the intuitive logic that justifies its existence.

Consider a single fully connected layer trained on Boolean input vectors \( X_1, X_2, \dots, X_b \) of size \( 1 \times n \) and their corresponding expected Boolean output vectors \( Y^e_1, Y^e_2, \dots, Y^e_b \) of size \( 1 \times m \). For the given input vectors, the layer produces the corresponding output vectors \( Y_1, Y_2, \dots, Y_b \). We then compute the output errors as \( E_1, E_2, \dots, E_b \), where \( E_k = Y_k \oplus Y^e_k \).

Our aim is to find \( D_w \) that minimizes the total Hamming weight (i.e., the number of 1s) in all the errors for subsequent inference after applying \( W' = W \oplus D_w \). Since each row \( W_i \) is processed independently of the others during inference, this task can clearly be reduced to a single row.

For a given i, assume we have all minimal difference masks \( D_k \) for each \( k \) that flip \( Y_{k,1,i} \) when using \( X_{k,1,i} \oplus D_k \) instead of \( X_{k,1,i} \). Now, let us construct two matrices by concatenating \( D_k \) as rows: \( C \) for indices where \( E_{k,1,i} = 0 \) and \( I \) for output vectors where \( E_{k,1,i} = 1 \). To construct a proper \( D_{w,i} \), we need to apply as many \( I \)-rows as possible to flip incorrect outputs while ensuring that \( C \)-rows are not activated, as their activation "spoils" correct outputs.

Here is a concrete example. Assume we have:
\[
    W = \begin{bmatrix}
        1 & 0 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 1 & 0 \\
        0 & 0 & 1 & 0 & 0 & 1
    \end{bmatrix},
    \quad
    B = \begin{bmatrix}0 & 1 & 0 \end{bmatrix}
\]

\[
    \begin{aligned}
        X_1 & = \begin{bmatrix} 1 & 1 & 0 & 1 & 0 & 1 \end{bmatrix}, \\
        X_2 & = \begin{bmatrix} 0 & 0 & 1 & 0 & 0 & 0 \end{bmatrix}, \\
        X_3 & = \begin{bmatrix} 0 & 0 & 0 & 0 & 0 & 1 \end{bmatrix}, \\
    \end{aligned}
    \quad
    \begin{aligned}
        Y^e_1 = \begin{bmatrix} 1 & 1 & 1 \end{bmatrix} \\
        Y^e_2 = \begin{bmatrix} 1 & 0 & 0 \end{bmatrix} \\
        Y^e_3 = \begin{bmatrix} 1 & 1 & 0 \end{bmatrix} \\
    \end{aligned}
\]

First, we compute outputs and errors:
\[
    \begin{aligned}
        Y_1 = \begin{bmatrix} 1 & 0 & 0 \end{bmatrix}, \\
        Y_2 = \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}, \\
        Y_3 = \begin{bmatrix} 1 & 1 & 1 \end{bmatrix}, \\
    \end{aligned}
    \quad
    \begin{aligned}
        E_1 = \begin{bmatrix} 0 & 1 & 1 \end{bmatrix} \\
        E_2 = \begin{bmatrix} 1 & 0 & 1 \end{bmatrix} \\
        E_3 = \begin{bmatrix} 0 & 0 & 1 \end{bmatrix} \\
    \end{aligned}
\]

Now we select the first element of the outputs to derive \( Dw_1 \) without loss of generality, as the remaining \( Dw_2 \) and \( Dw_3 \) can be obtained in the same manner.

It is easy to see that the minimal difference masks can be derived by selecting individual bits from the rows of the \textbf{Positive Activation Sensitivity} matrix or by taking entire rows from the \textbf{Negative Activation Sensitivity} matrix, depending on the corresponding \textbf{Row Activation} output values.

\[
    C = \begin{bmatrix}
        1 & 0 & 0 & 1 & 0 & 1 \\
        0 & 0 & 0 & 0 & 0 & 1
    \end{bmatrix}
    \quad
    I = \begin{bmatrix}
        1 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1
    \end{bmatrix}
\]
The matrix \( C \) above contains difference mask rows that "spoil" \( Y_{1,1,1} \) and \( Y_{3,1,1} \), while \( I \) consists of masks, each designed to "fix" \( Y_{2,1,1} \). It is clear that \( I_1 \) and \( I_2 \) do not "spoil" the outputs even when combined, but \( I_3 \) conflicts with \( C_2 \), and when combined with the former two, it also conflicts with \( C_1 \), so it should definitely be discarded. As a result, we get:
\[
    Dw_1 = \begin{bmatrix} 1 & 0 & 0 & 0 & 1 & 0 & 0 \end{bmatrix}
\]

After we have obtained the necessary intuition, we can formally define the \textbf{Error Projection} operation.

\begin{definition}
    \textbf{Error Projection} is an operation that is defined for two Boolean matrices, \( C \) of size \( p \times n \), and \( I \) of size \( q \times n \), denoted as:
    \[
        D = \mathcal{R}(C, I)
    \]
    and produces a \( 1 \times n \) Boolean row vector,
    \[
        D = \bigvee_{i=1}^{h} I'_{i},
    \]
    where \( I' \) is the largest possible subset of the rows of \( I \) such that
    \[
        \forall i, \quad C_i \wedge D \neq C_i.
    \]
\end{definition}



\section{Discussion}
Interpret results, compare with previous work, and explain implications.

\section{Conclusion}
Summarize the key points and suggest future directions.

\begin{thebibliography}{9}

    \bibitem{ma2024}
    Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei, \textit{The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits}, arXiv, 2024.

    \bibitem{rastegari2016}
    Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi, \textit{XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks}, arXiv, 2016.

\end{thebibliography}

\end{document}
